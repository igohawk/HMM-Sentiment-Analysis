{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some needed code from part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tuple(file):\n",
    "\n",
    "    data = []\n",
    "    lst = []\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line == \"\\n\":\n",
    "              data.append(lst)\n",
    "              lst = []\n",
    "            \n",
    "            else:\n",
    "              lines = line.replace(\"\\n\",'').split(\" \")\n",
    "              lst.append(tuple(lines))\n",
    "            \n",
    "    return data\n",
    "def smoothing_em(data, k):\n",
    "\n",
    "    new_data = []\n",
    "    new_tweets = []\n",
    "  \n",
    "    word_count = {}\n",
    "    \n",
    "    for lst in data:\n",
    "        for wordTuples in lst:\n",
    "          \n",
    "            word = wordTuples[0]\n",
    "            tag = wordTuples[1]\n",
    "            \n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "            if word_count[word] < k:\n",
    "              \n",
    "               new_tweets.append((\"#UNK#\",tag))\n",
    "                \n",
    "            else:\n",
    "              \n",
    "                new_tweets.append(wordTuples)\n",
    "                \n",
    "        new_data.append(new_tweets)\n",
    "        new_tweets = []\n",
    "        \n",
    "    return new_data,word_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def emission_parameter(data):\n",
    "    \n",
    "    tag_count = {}\n",
    "    match_count = {}\n",
    "    \n",
    "    for lst in data:         \n",
    "        for wordTuples in lst:\n",
    "            \n",
    "            tag = wordTuples[1]\n",
    "            \n",
    "            tag_count[tag] = tag_count.get(tag, 0) + 1\n",
    "            match_count[wordTuples] = match_count.get(wordTuples, 0) + 1\n",
    "            \n",
    "      \n",
    "    em = {i: match_count[i]/tag_count[i[1]] for i in match_count}   \n",
    "    return tag_count, em\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN part 3 done in 0.618115s\n",
      "EN part 3 done in 0.446967s\n",
      "EN part 3 done in 1.457448s\n",
      "EN part 3 done in 0.235937s\n"
     ]
    }
   ],
   "source": [
    "def create_transition_parameter(data):\n",
    "    tag_transition_count={}\n",
    "    #(tag0,tag1):count\n",
    "    tag_count = {}\n",
    "    #tag0:count\n",
    "    for tweet in data:\n",
    "        #data:list of (word,tag) tuples\n",
    "        for i in range(len(tweet)-1):\n",
    "            tag1 = tweet[i][1]\n",
    "            if i ==0:\n",
    "                tag0 = \"START\"\n",
    "            else:\n",
    "                tag0 = tweet[i-1][1]\n",
    "            tag_count[tag0]=tag_count.get(tag0,0)+1\n",
    "            tag_transition=(tag0,tag1)\n",
    "            tag_transition_count[tag_transition] = tag_transition_count.get(tag_transition,0)+1\n",
    "            if i == len(tweet)-1:\n",
    "                tag0 = tag[1]\n",
    "                tag[1] = \"STOP\"\n",
    "                tag_count[tag0]=tag_count.get(tag0,0)+1\n",
    "                tag_transition=(tag0,tag1)\n",
    "                tag_transition_count[tag_transition] = tag_transition_count.get(tag_transition,0)+1\n",
    "    transition_parameter = {k: tag_transition_count[k]/tag_count[k[0]]\n",
    "                        for k in tag_transition_count}\n",
    "    return transition_parameter\n",
    "\n",
    "def create_start_parameter(data):\n",
    "    start_tag_count={}\n",
    "    #tag:count\n",
    "    total_tweet_count =0\n",
    "    for tweet in data:\n",
    "        start_tag = tweet[0][1]\n",
    "        start_tag_count[start_tag]=start_tag_count.get(start_tag,0)+1\n",
    "        total_tweet_count += 1\n",
    "    start_parameter = {k: start_tag_count[k]/total_tweet_count\n",
    "                        for k in start_tag_count}\n",
    "    return start_parameter\n",
    "\n",
    "    \n",
    "        \n",
    "def viterbi(words, tags, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    for y in tags:\n",
    "        V[0][y] = start_p.get(y,0) * emit_p.get((words[0],y),0)\n",
    "        path[y] = [y]\n",
    "\n",
    "    for t in range(1,len(words)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for y in tags:\n",
    "            (prob, tag) = max([(V[t-1][y0] * trans_p.get((y0,y),0) * emit_p.get((words[t],y),0), y0) \n",
    "                               for y0 in tags])\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[tag] + [y]\n",
    "\n",
    "        path = newpath\n",
    "    (prob, tag) = max([(V[len(words) - 1][y], y) for y in tags])\n",
    "    return (prob, path[tag])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_hmm_prediction(country, part, prediction_function,\n",
    "                         word_sequence,tags, start_param, emit_param, trans_param):\n",
    "    \n",
    "    input_filename = country + \"/dev.in\"\n",
    "    output_filename = country + \"/dev.p\"+part+\".out\"\n",
    "    indata = []\n",
    "    with open(input_filename, \"r\") as infile:\n",
    "        indata = infile.read().strip('\\n').split('\\n\\n') \n",
    "    \n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        for tweet in indata:\n",
    "            word_sequence = tweet.split('\\n')\n",
    "            predicted_tag_sequence = prediction_function(word_sequence,\n",
    "                                                tags, start_param, trans_param, emit_param)[1]\n",
    "            if len(word_sequence) != len(predicted_tag_sequence):\n",
    "                print(\"WARNING!! Different length {} / {}\"\\\n",
    "                      .format(word_sequence, predicted_tag_sequence))\n",
    "            for i in range(len(word_sequence)):\n",
    "                line = \"{} {}\\n\".format(word_sequence[i], \n",
    "                                        predicted_tag_sequence[i])\n",
    "                outfile.write(line)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "folders = [\"CN (1)\", \"EN\", \"SG(1)\", \"FR\"]\n",
    "for folder in folders:\n",
    "    start = datetime.now()\n",
    "    data = data_tuple(folder+\"/train\")\n",
    "    supress_data,words = smoothing_em(data,3)\n",
    "\n",
    "    tags,emit_param = emission_parameter(supress_data)[0].keys(),emission_parameter(supress_data)[1]\n",
    "    trans_param = create_transition_parameter(supress_data)\n",
    "    start_param = create_start_parameter(supress_data)\n",
    "    write_hmm_prediction(c,\"3\", viterbi,\n",
    "                        words, tags, start_param, emit_param, trans_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 3 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR part 5 done in 1.339318s\n",
      "EN part 5 done in 1.11680s\n"
     ]
    }
   ],
   "source": [
    "#perceptron for part 5\n",
    "from collections import defaultdict\n",
    "import random, os\n",
    "\n",
    "\n",
    "def read_unlabel_file(folder_path, filename):\n",
    "    tweets = []\n",
    "    with open(os.path.join(folder_path, filename), 'r', encoding=\"utf8\") as infile:\n",
    "        for line in infile:\n",
    "            tweet = line.strip()\n",
    "            if tweet!=\"\":\n",
    "                tweets.append(tweet)\n",
    "            else:\n",
    "                tweets.append(\"\")\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def read_label_file(folder_path, filename):\n",
    "    tweets = []\n",
    "    tag_counts = {}\n",
    "    with open(os.path.join(folder_path, filename), 'r', encoding=\"utf8\") as infile:\n",
    "        for line in infile:\n",
    "            if line.strip()!=\"\":\n",
    "                tweet = line.strip().split(\" \")\n",
    "                tweets.append((tweet[0],tweet[1]))\n",
    "                tag_counts[tweet[1]] = tag_counts.get(tweet[1], 0) + 1\n",
    "            else:\n",
    "                tweets.append((\"\",\"\"))\n",
    "    return tweets,tag_counts\n",
    "\n",
    "n=10\n",
    "class perceptron():\n",
    "    \n",
    "    def __init__(self, tag_counts):\n",
    "        self.tags = {}\n",
    "        self.classes=tag_counts.keys()\n",
    "        self.tag_counts = tag_counts\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self,feature):\n",
    "        scores = defaultdict(float)\n",
    "        if feature !=\"\":\n",
    "            if feature in self.tags:\n",
    "                tags = self.tags[feature]\n",
    "                for clas, tag in tags.items():\n",
    "                    scores[clas] +=tag\n",
    "            return max(self.classes, key=lambda clas: (scores[clas], clas))\n",
    "        return \"\"\n",
    "\n",
    "    def train(self, n_iter, examples):\n",
    "        for i in range(n_iter):\n",
    "            for features, ntag in examples:\n",
    "                if features!=\"\":\n",
    "                    guess = self.predict(features)\n",
    "                    if guess != ntag:\n",
    "                        if features not in self.tags:\n",
    "                            self.tags[features]={}\n",
    "                        \n",
    "                        self.tags[features][ntag] = self.tags[features].get(ntag,0) + 1\n",
    "                        self.tags[features][guess] = self.tags[features].get(guess,0) - 1\n",
    "                        \n",
    "            random.shuffle(examples)\n",
    "\n",
    "\n",
    "\n",
    "folders = [\"FR\",\"EN\"]\n",
    "\n",
    "for folder in folders:\n",
    "    start = datetime.now()\n",
    "    feature_pairs, weight_counts = read_label_file(folder, \"train\")\n",
    "    test = perceptron(weight_counts)\n",
    "    test.train(n, feature_pairs)\n",
    "    with open(os.path.join(folder, \"test.p5.out\"), 'w', encoding=\"utf8\") as outfile:\n",
    "        for feature in read_unlabel_file(folder, \"test.in\"):\n",
    "            outfile.write(feature + \" \" + test.predict(feature)+\"\\n\")\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 5 done in {}.{}s\"\\\n",
    "          .format(folder, delt.seconds, delt.microseconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
